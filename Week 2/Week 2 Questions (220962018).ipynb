{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8e0372d-2bfa-4eac-aa27-589f48c3e228",
   "metadata": {},
   "source": [
    "## Week 2 Lab Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0e18403-a044-4172-b555-ed3e4e46cda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.device_count() , device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be34079-afce-456a-83b1-b89e3b1b931c",
   "metadata": {},
   "source": [
    "## Lab Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33765de-e4e5-4eaa-a209-28d479587a21",
   "metadata": {},
   "source": [
    "### 1) Draw Computation Graph and work out the gradient dz/da by following the path back from z to a and compare the result with the analytical gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29a7ed8-d60d-4b50-b602-e7a6c96d383b",
   "metadata": {},
   "source": [
    "$$\n",
    "x = 2a + 3b\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = 5a^2 + 3b^3\n",
    "$$\n",
    "\n",
    "$$\n",
    "z = 2x + 3y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9694d3e6-b56a-4b85-8e34-be9d45bf1499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of z with respect to a: 34.0\n",
      "Analytical Solution:  34.0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "x = 2*a + 3*b\n",
    "y = 5*a**2 + 3*b**3\n",
    "z = 2*x + 3*y\n",
    "\n",
    "z.backward() \n",
    "\n",
    "print(f\"Gradient of z with respect to a: {a.grad.item()}\")\n",
    "\n",
    "print(\"Analytical Solution: \",(4+ 30 * a).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b8a49-0bef-44ed-a080-374e49a9a198",
   "metadata": {},
   "source": [
    "### 2) For the following Computation Graph, work out the gradient da/dw by following the path back from a to w and compare the result with the analytical gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05ba8fb4-87f7-47fb-9fcb-6c4349736e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of a with respect to w: 1.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)  \n",
    "b = torch.tensor(1.0, requires_grad=True)  \n",
    "w = torch.tensor(1.0, requires_grad=True)  \n",
    "\n",
    "u = w * x\n",
    "v = u + b\n",
    "a = torch.relu(v)\n",
    "\n",
    "a.backward()\n",
    "\n",
    "print(f\"Gradient of a with respect to w: {w.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacb1d4f-a03d-485e-a7ad-61e77fac0da9",
   "metadata": {},
   "source": [
    "### 3) Repeat the Problem 2 using Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f3744ad-9153-4d9f-831b-4eb65ccb3276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of a with respect to w: 0.10499362647533417\n",
      "Analytical Solution:  0.10499362647533417\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)  \n",
    "b = torch.tensor(1.0, requires_grad=True)  \n",
    "w = torch.tensor(1.0, requires_grad=True) \n",
    "\n",
    "u = w * x\n",
    "v = u + b\n",
    "a = torch.sigmoid(v)\n",
    "\n",
    "a.backward()\n",
    "\n",
    "print(f\"Gradient of a with respect to w: {w.grad.item()}\")\n",
    "\n",
    "print(\"Analytical Solution: \",(a*(1-a)*x).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8993d60-eb28-490b-bd8c-c3eb5a87d843",
   "metadata": {},
   "source": [
    "### 4) Verify that the gradients provided by PyTorch match with the analytical gradients of the function f= exp(-x2-2x-sin(x)) w.r.t x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc867f2e-8d4b-4b85-b287-26da034c3770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of f with respect to x: -0.09744400531053543\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "u = x**2\n",
    "v = 2*x\n",
    "w = torch.sin(x)\n",
    "f = torch.exp(-u - v - w)\n",
    "\n",
    "f.backward()\n",
    "\n",
    "print(f\"Gradient of f with respect to x: {x.grad.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd180f11-34ae-48cf-b0b9-292a0fc2ef9a",
   "metadata": {},
   "source": [
    "### 5) Compute gradient for the function y=8x4+ 3x3 +7x2+6x+3 and verify the gradients provided by PyTorch with the analytical gradients. A snapshot of the Python code is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32a925c0-94a7-45ba-8e8b-6e94e446e524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of y with respect to x: 326.0\n",
      "Analytical Solution:  326.0\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "y=8*x**4+3*x**3+7*x**2+6*x+3\n",
    "y.backward()\n",
    "\n",
    "print(f\"Gradient of y with respect to x: {x.grad.item()}\")\n",
    "\n",
    "print(\"Analytical Solution: \",(32*x**3+9*x**2+14*x+6).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e3e3fd-028e-42e7-9f1c-37f332af2d8d",
   "metadata": {},
   "source": [
    "### Calculate the intermediate variables a,b,c,d, and e in the forward pass. Starting from f, calculate the gradient of each expression in the backward pass manually. Calculate ∂f/∂y using the computational graph and chain rule. Use the chain rule to calculate gradient and compare with analytical gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd8ce983-235e-4bb2-8d3c-7c062edc1d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b (sin(y)): 0.8414709568023682\n",
      "a (2 * x): 2.0\n",
      "c (a / b): 2.3767902851104736\n",
      "d (c * z): 2.3767902851104736\n",
      "e (log(d + 1)): 1.2169256210327148\n",
      "f (tanh(e)): 0.8387449383735657\n",
      "Gradient of f with respect to y: -0.13400448858737946\n",
      "Analytical Solution:  -0.13400450348854065\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(1.0, requires_grad=True)\n",
    "z = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "b = torch.sin(y)\n",
    "print(f\"b (sin(y)): {b.item()}\")\n",
    "\n",
    "a = 2 * x\n",
    "print(f\"a (2 * x): {a.item()}\")\n",
    "\n",
    "c = a / b\n",
    "print(f\"c (a / b): {c.item()}\")\n",
    "\n",
    "d = c * z\n",
    "print(f\"d (c * z): {d.item()}\")\n",
    "\n",
    "e = torch.log(d + 1)\n",
    "print(f\"e (log(d + 1)): {e.item()}\")\n",
    "\n",
    "f = torch.tanh(e)\n",
    "print(f\"f (tanh(e)): {f.item()}\")\n",
    "\n",
    "f.backward()\n",
    "\n",
    "\n",
    "print(f\"Gradient of f with respect to y: {y.grad.item()}\")\n",
    "\n",
    "print(\"Analytical Solution: \",((1-torch.tanh(e)**2)*(1/(d+1))*(-(a/b**2))*torch.cos(y)).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
